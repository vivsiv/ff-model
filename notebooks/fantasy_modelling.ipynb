{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "- Scrape 20 years of data from pro football reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraper import ProFootballReferenceScraper\n",
    "\n",
    "scraper = ProFootballReferenceScraper(data_dir=\"../data\")\n",
    "scraper.scrape_years(start_year=1999, end_year=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "- Combine the stat tables for all years and save them to the silver layer\n",
    "- Build the gold table from the different stat tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files matching: *_player_fantasy_stats.csv: 100%|██████████| 25/25 [00:00<00:00, 280.41it/s]\n",
      "2025-07-30 09:06:44,159 - src.processor - INFO - Saved player_fantasy_stats.csv to ../data/silver/player_fantasy_stats.csv\n",
      "Processing files matching: *_player_receiving_stats.csv: 100%|██████████| 25/25 [00:00<00:00, 196.19it/s]\n",
      "2025-07-30 09:06:50,489 - src.processor - INFO - Saved player_receiving_stats.csv to ../data/silver/player_receiving_stats.csv\n",
      "Processing files matching: *_player_rushing_stats.csv: 100%|██████████| 25/25 [00:00<00:00, 196.45it/s]\n",
      "2025-07-30 09:06:55,160 - src.processor - INFO - Saved player_rushing_stats.csv to ../data/silver/player_rushing_stats.csv\n",
      "Processing files matching: *_player_passing_stats.csv:   0%|          | 0/25 [00:00<?, ?it/s]2025-07-30 09:06:55,166 - src.processor - INFO - Added missing pass_qbr column to ../data/bronze/2001_player_passing_stats.csv\n",
      "2025-07-30 09:06:55,169 - src.processor - INFO - Added missing pass_qbr column to ../data/bronze/2005_player_passing_stats.csv\n",
      "2025-07-30 09:06:55,188 - src.processor - INFO - Added missing pass_qbr column to ../data/bronze/2004_player_passing_stats.csv\n",
      "2025-07-30 09:06:55,191 - src.processor - INFO - Added missing pass_qbr column to ../data/bronze/2000_player_passing_stats.csv\n",
      "2025-07-30 09:06:55,201 - src.processor - INFO - Added missing pass_qbr column to ../data/bronze/2002_player_passing_stats.csv\n",
      "Processing files matching: *_player_passing_stats.csv: 100%|██████████| 25/25 [00:00<00:00, 465.01it/s]\n",
      "2025-07-30 09:06:57,662 - src.processor - INFO - Saved player_passing_stats.csv to ../data/silver/player_passing_stats.csv\n",
      "Processing files matching: *_team_offense.csv: 100%|██████████| 25/25 [00:00<00:00, 1131.67it/s]\n",
      "2025-07-30 09:06:57,719 - src.processor - INFO - Saved team_offense.csv to ../data/silver/team_offense.csv\n",
      "2025-07-30 09:06:58,405 - src.processor - INFO - Final data saved to ../data/gold/training_set.csv\n",
      "2025-07-30 09:06:58,525 - src.processor - INFO - Final data saved to ../data/gold/live_set.csv\n"
     ]
    }
   ],
   "source": [
    "from src.processor import DataProcessor\n",
    "\n",
    "processor = DataProcessor(data_dir=\"../data\")\n",
    "processor.process_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis \n",
    "\n",
    "- Do some data quality checks on the final stats\n",
    "- Look at which features are the most and least informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 09:18:00,700 - src.analysis - INFO - Loaded training data: 9431 rows\n",
      "2025-07-30 09:18:00,706 - src.analysis - INFO - Loaded live data: 374 rows\n"
     ]
    }
   ],
   "source": [
    "from src.analysis import DataAnalysis\n",
    "\n",
    "analysis = DataAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.run_training_data_quality_checks()\n",
    "analysis.run_live_data_quality_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis.generate_feature_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "- Start with linear regression\n",
    "- Become more advanced\n",
    "- Forward, Backward, Feature Selection\n",
    "- Models per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import FantasyModel\n",
    "\n",
    "model = FantasyModel(data_dir=\"../data\", target_col=\"ppr_fantasy_points_per_game\")\n",
    "# model = FantasyModel(data_dir=\"../data\", target_col=\"standard_fantasy_points_per_game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = model.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_search = model.run_model_eval(data)\n",
    "\n",
    "model_eval_results_df = pd.DataFrame(model_eval_search.cv_results_)\n",
    "\n",
    "display(model_eval_results_df[['param_model', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']].sort_values(by='mean_test_r2', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "ridge_eval_results_df = pd.DataFrame(ridge_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "random_forest_eval_results_df = pd.DataFrame(random_forest_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'param_model__max_depth' 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_eval_search = model.run_model_tuning(data, \"svr\")\n",
    "\n",
    "svr_eval_results_df = pd.DataFrame(svr_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__C', 'param_model__kernel', 'param_model__gamma', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = model.make_test_predictions(data, \"ridge\")\n",
    "\n",
    "view_year = 2024\n",
    "year_preds = model.view_year_test_predictions(preds_df, view_year)\n",
    "year_preds.to_csv(os.path.join(model.predictions_dir, f\"{model.target_col}_{view_year}_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"Predictions for {model.target_col} in {view_year}:\")\n",
    "print(year_preds.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
