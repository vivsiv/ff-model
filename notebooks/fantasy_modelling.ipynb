{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "- Scrape 20 years of data from pro football reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraper import ProFootballReferenceScraper\n",
    "\n",
    "scraper = ProFootballReferenceScraper(data_dir=\"../data\")\n",
    "scraper.scrape_years(start_year=1999, end_year=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "- Combine the stat tables for all years and save them to the silver layer\n",
    "- Build the gold table from the different stat tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.processor import FantasyDataProcessor\n",
    "\n",
    "processor = FantasyDataProcessor(data_dir=\"../data\", drop_rookies=True)\n",
    "processor.process_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis \n",
    "\n",
    "- Do some data quality checks on the final stats\n",
    "- Do some sanity checks on the final stats\n",
    "- Look at which features are the most and least informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_analysis import DataAnalysis\n",
    "\n",
    "data_analysis = DataAnalysis(\n",
    "    data_dir=\"../data\",\n",
    "    metadata_cols=[\"id\"],\n",
    "    target_cols=[\"standard_fantasy_points\", \"standard_fantasy_points_per_game\", \"ppr_fantasy_points\", \"ppr_fantasy_points_per_game\", \"value_over_replacement\"],\n",
    ")\n",
    "\n",
    "final_stats = data_analysis.gold_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See shape\n",
    "print(final_stats.shape)\n",
    "\n",
    "# Check null values\n",
    "null_values = final_stats.isnull().sum()\n",
    "print(f\"Rows with null values: {null_values[null_values > 0].count()}\")\n",
    "\n",
    "# Check data types\n",
    "data_types = final_stats.dtypes\n",
    "print(f\"Non float data types: {data_types[data_types != 'float64'].count()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows in final stats: {final_stats.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that games are joined correctly (rookies should have 0 game played)\n",
    "print(final_stats[final_stats['id'] == 'malik_nabers_2024'][['id', 'age','games', 'games_2_yr_avg', 'games_3_yr_avg']])\n",
    "\n",
    "# Check that the correct year in player fantasy stats is joined with the correct year in player stats\n",
    "\n",
    "# Check that the last year of data is dropped\n",
    "\n",
    "# TODO: brainstorm other sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_analysis.generate_feature_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "- Start with linear regression\n",
    "- Become more advanced\n",
    "- Forward, Backward, Feature Selection\n",
    "- Models per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import FantasyModel\n",
    "\n",
    "model = FantasyModel(data_dir=\"../data\", target_col=\"ppr_fantasy_points_per_game\")\n",
    "# model = FantasyModel(data_dir=\"../data\", target_col=\"standard_fantasy_points_per_game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = model.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_search = model.run_model_eval(data)\n",
    "\n",
    "model_eval_results_df = pd.DataFrame(model_eval_search.cv_results_)\n",
    "\n",
    "display(model_eval_results_df[['param_model', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']].sort_values(by='mean_test_r2', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "ridge_eval_results_df = pd.DataFrame(ridge_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "random_forest_eval_results_df = pd.DataFrame(random_forest_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'param_model__max_depth' 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_eval_search = model.run_model_tuning(data, \"svr\")\n",
    "\n",
    "svr_eval_results_df = pd.DataFrame(svr_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__C', 'param_model__kernel', 'param_model__gamma', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = model.make_test_predictions(data, \"ridge\")\n",
    "\n",
    "view_year = 2024\n",
    "year_preds = model.view_year_test_predictions(preds_df, view_year)\n",
    "year_preds.to_csv(os.path.join(model.predictions_dir, f\"{model.target_col}_{view_year}_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"Predictions for {model.target_col} in {view_year}:\")\n",
    "print(year_preds.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
