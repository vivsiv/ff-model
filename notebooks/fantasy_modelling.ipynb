{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "- Scrape 20 years of data from pro football reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scraper import ProFootballReferenceScraper\n",
    "\n",
    "scraper = ProFootballReferenceScraper(data_dir=\"../data\")\n",
    "scraper.scrape_years(start_year=1999, end_year=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "- Combine the stat tables for all years and save them to the silver layer\n",
    "- Build the gold table from the different stat tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.processor import DataProcessor\n",
    "\n",
    "processor = DataProcessor(data_dir=\"../data\")\n",
    "processor.process_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis \n",
    "\n",
    "- Do some data quality checks on the final stats\n",
    "- Look at which features are the most and least informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import DataAnalysis\n",
    "\n",
    "analysis = DataAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.run_training_data_quality_checks()\n",
    "analysis.run_live_data_quality_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis.generate_feature_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "- Start with linear regression\n",
    "- Become more advanced\n",
    "- Forward, Backward, Feature Selection\n",
    "- Models per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import FantasyModel\n",
    "\n",
    "model = FantasyModel(data_dir=\"../data\", target_col=\"ppr_fantasy_points_per_game\")\n",
    "# model = FantasyModel(data_dir=\"../data\", target_col=\"standard_fantasy_points_per_game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = model.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_search = model.run_model_eval(data)\n",
    "\n",
    "model_eval_results_df = pd.DataFrame(model_eval_search.cv_results_)\n",
    "\n",
    "display(model_eval_results_df[['param_model', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']].sort_values(by='mean_test_r2', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "ridge_eval_results_df = pd.DataFrame(ridge_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_eval_search = model.run_model_tuning(data, \"random_forest\")\n",
    "\n",
    "random_forest_eval_results_df = pd.DataFrame(random_forest_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__n_estimators', 'param_model__max_depth' 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_eval_search = model.run_model_tuning(data, \"svr\")\n",
    "\n",
    "svr_eval_results_df = pd.DataFrame(svr_eval_search.cv_results_)\n",
    "\n",
    "display(\n",
    "    ridge_eval_results_df[['param_model__C', 'param_model__kernel', 'param_model__gamma', 'mean_test_r2', 'mean_test_rmse', 'std_test_r2']]\n",
    "    .sort_values(by=['mean_test_r2', 'mean_test_rmse'], ascending=[False, True])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_df = model.make_test_predictions(data, \"ridge\")\n",
    "\n",
    "view_year = 2024\n",
    "year_test_preds = model.view_year_test_predictions(test_preds_df, view_year)\n",
    "year_test_preds.to_csv(os.path.join(model.predictions_dir, f\"{model.target_col}_{view_year}_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"Predictions for {model.target_col} in {view_year}:\")\n",
    "print(year_preds.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_preds_df = model.make_live_predictions(data, \"ridge\")\n",
    "\n",
    "live_preds_df.to_csv(os.path.join(model.predictions_dir, f\"{model.target_col}_live_predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"Live predictions for {model.target_col}:\")\n",
    "print(live_preds_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
